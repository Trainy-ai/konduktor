kubernetes:
  pod_config:
    metadata:
      trainy.ai/job-name: {{ job_name }}
      trainy.ai/konduktor-managed: true
    spec:
      # trigger this on GPU request
      {% if num_gpus is not none %}
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
      {% endif %}
      containers:
        # TODO(asaiacai): should decide here whether we add the fabric interfaces/containers init etc.
        - name: konduktor-container
          image: {{ image_id }}
          # this is set during jobset definition since we need to know the jobset
          # name and number of nodes to set all the environment variables correctly here
          # as well as the additional from the job definition
          env: []
          command: ["bash", "-c"]
          # TODO(asaiacai): should we just mount this as a configmap instead?
          args:
            - |
              {{ run_cmd | indent( width=14 ) }}

      resources:
        limits:
          cpu: {{ cpu }}
          memory: {{ memory }}
          # TODO(asaiacai): need to decide whether we include fabric configuration here
          {% if num_gpus is not none %}
          nvidia.com/gpu: {{ num_gpus }}
          {% endif %}
        requests:
          cpu: {{ cpu }}
          memory: {{ memory }}
          {% if num_gpus is not none %}
          nvidia.com/gpu: {{num_gpus}}
          {% endif %}

        
        
      # TODO(asaiacai): should we add nodeSelectors here or leave to
      # kueue resource flavors. leaning towards defining
      # in kueue and just querying for the kueue resource flavor
